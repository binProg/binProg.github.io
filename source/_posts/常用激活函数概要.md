---
title: 常用激活函数概要
date: 2021-12-15 20:48:27
tags: nlp 激活函数
mathjax: true
---

# 常用激活函数概要

**激活函数作用：**

> 考虑到真实世界，大多数系统是**非线性**的，如果要模拟复杂系统则必须借助非线性的激活函数！

**通用近似理论(Universal Approximation Theorem):** 

> 神经网络中至少需要一层隐藏层和足够的神经元，利用非线性的激活函数，便可以模拟任何复杂的非线性函数。

## 1. Sigmoid函数

公式：

$$ sigmoid(x) = \frac{1}{1+e^{-x}} $$

<img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fs5.51cto.com%2Fwyfs02%2FM01%2F85%2F51%2FwKiom1egBxOBiT26AABfVHYjxlU640.jpg&refer=http%3A%2F%2Fs5.51cto.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1642147041&t=2380c142f902867afbde4a2b9d36a1df" height="300px">

特点：

- Sigmoid的取值范围在$(0,1)$之间
- Sigmoid函数的导值在$(0,0.25)$的连续区间

缺点：

1. 在神经网络模型，Sigmoid函数存在**"梯度消失"问题**：

   在逆向参数调整（Back Propagation）过程中，使用链式法则（Chain Rule）可以推导出如下图所示的公式：

   <img src="https://s4.ax1x.com/2021/12/15/TSG6eg.png" height="150px">

   基于上图公式，对深度神经网络的权重调整幅度进行计算，上图中间部分是多个Sigmoid导数的乘积，多个0.25相乘后$(0.25*0.25*0.25*\cdots*0.25)​$，权重逐渐趋于0，即导致”梯度消失“。

2. Sigmoid函数在进行指数计算时会消耗较多的计算资源 

## 2. tanh函数

公式：
$$
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$
<img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fwww.soolco.com%2Fgroup1%2FM00%2F0E%2FEF%2FrBAADF-8cmqAP7ZBAACAfZwqipg022.png&refer=http%3A%2F%2Fwww.soolco.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1642147920&t=e8fbccac442127ec2dfd2125e1950d3e" height="400px">

特点：

- tanh函数的值位于$(-1, 1)$区间
- tanh函数的导数值位于$(0, 1)$区间，
  - 因为tanh的导数值范围大于sigmoid，所以tanh显著缓解了梯度消失问题，但不能消除！

## 3. ReLU函数

公式：
$$
relu(x) = max(0, x)
$$
<img src="https://img0.baidu.com/it/u=3033173174,2106807370&fm=26&fmt=auto" height="300px">

**隐藏层的默认推荐函数：ReLU**

特点：

- ReLU在$x>0$时显示的线性特征，能够很好地解决梯度消失问题
- 根据通用相似定理，ReLU函数整体的非线性又能够在神经网络中拟合任何复杂的连续函数
- 当$x < 0$时，其函数值和导数值均为0，在逆向梯度调整时，不产生参数值的改变

